repos = 'https://macos.rbind.org'
)
})
install.packages(c('RGtk2', 'cairoDevice', 'rattle'))
library(rattle)
install.packages("rattle")
MyData$Accident_Prob <- predict(mytree, newdata = MyData, type="prob")
install.packages("ggplot2")
install.packages("magrittr")
install.packages("RGtk2")
# Data set before being split
MyData <- read.csv(file="/Users/timothylytkine/Desktop/Predictive Modeling Project/permits_accident_data_2013_Present.csv", header=TRUE, sep=",")
# Copies of original data set to use later
MyData1 <- MyData
MyData2 <- MyData
MyData3 <- MyData
# load r part library
library(rpart)
View(testing)
View(training)
# use copy of original data set to split into training and testing data set
training_set <- sample_frac(MyData1,0.8)
sid<-as.numeric(rownames(training_set))
testing_set<-MyData1[-sid,]
# 80 percent of data for training
training_set
# 20 percent of data for testing
testing_set
mytree <- rpart(Accident ~ Proposed.Stories + Job.Type + Borough + Square.Footage + General.Contractor + Cost.Estimate , data = training_set, method = "class", minsplit = 2, minbucket = 1)
# use copy of original data set to split into training and testing data set
training_set <- sample_frac(MyData1,0.8)
# load dplyr library
library(dplyr)
# use copy of original data set to split into training and testing data set
training_set <- sample_frac(MyData1,0.8)
mytree <- rpart(Accident ~ Proposed.Stories + Job.Type + Borough + Square.Footage + General.Contractor + Cost.Estimate , data = training_set, method = "class", minsplit = 2, minbucket = 1)
training_set$Accident_Pred <- predict(mytree, newdata = training_set, type="class")
training_set$Accident_Pred <- predict(mytree, newdata = training_set, type="prob")
printcp(mytree)
mytree
View(testing_set)
View(training_set)
printcp(mytree)
# Data set before being split
MyData <- read.csv(file="/Users/timothylytkine/Desktop/Predictive Modeling Project/permits_accident_data_2013_Present.csv", header=TRUE, sep=",")
# Copies of original data set to use later
MyData1 <- MyData
MyData2 <- MyData
MyData3 <- MyData
# load r part library
library(rpart)
# create tree using unsplit data set
mytree <- rpart(Accident ~ Proposed.Stories + Job.Type + Borough + Square.Footage + General.Contractor + Cost.Estimate , data = MyData, method = "class", minsplit = 2, minbucket = 1)
# Syntax for binary prediction and probability
# unsplit data set
MyData$Accident_Pred <- predict(mytree, newdata = MyData, type="class")
MyData$Accident_Pred <- predict(mytree, newdata = MyData, type="prob")
printcp(mytree)
# load dplyr library
library(dplyr)
# use copy of original data set to split into training and testing data set
training_set <- sample_frac(MyData1,0.8)
sid<-as.numeric(rownames(training_set))
testing_set<-MyData1[-sid,]
# 80 percent of data for training
training_set
# 20 percent of data for testing
testing_set
library(rattle)
library(rpart.plot)
library(RColorBrewer)
test_tree <- rpart(Accident ~ Proposed.Stories + Job.Type + Borough + Square.Footage + General.Contractor + Cost.Estimate , data = training_set, method = "class", minsplit = 2, minbucket = 1)
training_set$Accident_Pred <- predict(test_tree, newdata = training_set, type="class")
training_set$Accident_Pred <- predict(test_tree, newdata = training_set, type="prob")
printcp(test_tree)
test_tree
summary(test_tree)
fitness <- read.table("/Users/timothylytkine/Desktop/Work/Decision-Tree-Model-in-R-/fitnessAppLog.csv",sep=",",header=T)
library("rpart")
treeAnalysis <- rpart(PayOrNot~Incomes+GymVisits+State,data=fitness)
treeAnalysis
install.packages("rpart.plot")
library("rpart.plot")
rpart.plot(treeAnalysis,extra=4)
ls()
rm()
ls
ls()
View(fitness)
First()
ls()
rm(fitness)
ls()
rn(MyData)
rm(all)
rm(MyData)
rm(MyData1)
rm(MyData2)
ls()
rm(MyData3)
ls
ls
ls9)
ls()
rm(mytree)
rm(permits_accident_data_2013_Present)
ls()
ls
ls()
rm(n)
rm(train)
rm(z)
rm(sid)
rm(training)
rm(test)
rm(training_set)
rm(test_tree)
rm(treeAnalysis)
rm(testing)
rm(x)
rm(testing_set)
rm(y)
ls()
rm(character(0))
ls()
a = 49
sqrt(a)
a = "The dog ate my homework"
sub("dog","cat",a)
a = (1+1==3)
a
a = c(1,2,3)
a*2
a<-matrix(data=0,nr=1,nc=3)
a
doe = list(name="john",age=28,married=F)
doe$name
doe$age
a
addPercent <- function(x)
{
percent <-round(x*100,digits=1)
result <- paste(percent,"%",sep="")
return result
}
addPercent <- function(x)
{
percent <-round(x*100,digits=1)
result <- paste(percent,"%",sep="")
return(result)
}
install.packages("readr")
library(readr)
while(1){
print("Yo")
}
# take input from the user
num = as.integer(readline(prompt="Enter a number: "))
# check if the number is negative, positive
if (num < 0)
{
print("Sorry, factorial does not exist for negative numbers")
}
else if(num == 0)
{
print("The factorial of 0 is 1")
}
else
{
for(i in 1:num)
{
factorial = factorial * i
}
}
print(paste("The factorial of",num,"is",factorial))
}
# take input from the user
num = as.integer(readline(prompt="Enter a number: "))
factorial = 1
# check if the number is negative, positive
if (num < 0)
{
print("Sorry, factorial does not exist for negative numbers")
}
else if(num == 0)
install.packages("readr")
library(readr)
install.packages("dplyr")
install.packages("dplyr")
install.packages(ggplot2)
install.packages(ggplot)
install.packages("ggplot2")
installed.packages("esquisse")
test_expression = "yes"
if(test_expression){
print("Yes")
} else {
print("No")
}
test_expression = "yes"
if (test_expression){
print("Yes")
} else {
print("No")
}
test_expression = TRUE
if (test_expression){
print("Yes")
} else {
print("No")
}
test_expression = TRUE
if (test_expression){
print("Yes")
} else {
print("No")
}
install.packages("igraph")
install.packages("sna")
plot(a)
plot(doe)
library(data.table)
library(ggplot2)
library(gridExtra)
library(corrplot)
library(rpart)
library(randomForest)
library(stepPlr)
library(C50)
library(plyr)
library(MASS)
library(caret)
library(caretEnsemble)
library(dplyr)
library(plotly)
# make subplots
p <- subplot(
# histogram (counts) of gear
plot_ly(d, x = ~factor(gear)) %>%
add_histogram(color = I("grey50")),
# scatterplot of disp vs mpg
scatterplot,
titleX = T
)
# define a shared data object
d <- SharedData$new(mtcars)
g <- ggplot(txhousing, aes(x = date, y = sales, group = city)) +
geom_line(alpha = 0.4)
g
ggplotly(g, tooltip = c("city"))
g <- txhousing %>%
# group by city
group_by(city) %>%
# initiate a plotly object with date on x and median on y
plot_ly(x = ~date, y = ~median) %>%
# add a line plot for all texan cities
add_lines(name = "Texan Cities", hoverinfo = "none",
type = "scatter", mode = "lines",
line = list(color = 'rgba(192,192,192,0.4)')) %>%
# plot separate lines for Dallas and Houston
add_lines(name = "Houston",
data = filter(txhousing,
city %in% c("Dallas", "Houston")),
hoverinfo = "city",
line = list(color = c("red", "blue")),
color = ~city)
g
library(crosstalk)
# define a shared data object
d <- SharedData$new(mtcars)
# make a scatterplot of disp vs mpg
scatterplot <- plot_ly(d, x = ~mpg, y = ~disp) %>%
add_markers(color = I("navy"))
# define two subplots: boxplot and scatterplot
subplot(
# boxplot of disp
plot_ly(d, y = ~disp) %>%
add_boxplot(name = "overall",
color = I("navy")),
# scatterplot of disp vs mpg
scatterplot,
shareY = TRUE, titleX = T) %>%
layout(dragmode = "select")
# make subplots
p <- subplot(
# histogram (counts) of gear
plot_ly(d, x = ~factor(gear)) %>%
add_histogram(color = I("grey50")),
# scatterplot of disp vs mpg
scatterplot,
titleX = T
)
layout(p, barmode = "overlay")
library(networkD3)
data(MisLinks, MisNodes)
head(MisLinks, 3)
head(MisNodes, 3)
forceNetwork(Links = MisLinks, Nodes = MisNodes, Source = "source",
Target = "target", Value = "value", NodeID = "name",
Group = "group", opacity = 0.9, Nodesize = 3,
linkDistance = 100, fontSize = 20)
# Scatterplot
gg <- ggplot(midwest, aes(x=area, y=poptotal)) +
geom_point(aes(col=state, size=popdensity)) +
geom_smooth(method="loess", se=F) +
xlim(c(0, 0.1)) +
ylim(c(0, 500000)) +
labs(subtitle="Area Vs Population",
y="Population",
x="Area",
title="Scatterplot",
caption = "Source: midwest")
plot(gg)
install.packages("cowplot")  # a gganimate dependency
install.packages("devtools")
devtools::install_github("https://github.com/thomasp85/gganimate/releases/tag/v0.1.1")
install.packages("gapminder")
library(ggplot2)
library(gganimate)
library(gapminder)
theme_set(theme_bw())  # pre-set the bw theme.
g <- ggplot(gapminder, aes(gdpPercap, lifeExp, size = pop, frame = year)) +
geom_point() +
geom_smooth(aes(group = year),
method = "lm",
show.legend = FALSE) +
facet_wrap(~continent, scales = "free") +
scale_x_log10()  # convert to log scale
gganimate(g, interval=0.2)
setwd("~/Desktop/DataScience/BigData/Projects/Project3BigData")
# Load required libraries
library(devtools)
library(textreuse)
library(SnowballC)
library(wordcloud)
library(NLP)
library(rJava)
library(wordnet)
library(tm)
library(zipfR)
library(quanteda)
setwd("~/Desktop/DataScience/BigData/Projects/Project3BigData")
# Load required libraries
library(devtools)
library(textreuse)
library(SnowballC)
library(wordcloud)
library(NLP)
library(rJava)
library(wordnet)
library(tm)
library(zipfR)
library(quanteda)
library(stringi)
library(syuzhet)
library(corpus)
library(openNLP)
library(readr)
library(stringr)
library(languageR)
library(RWeka)
library(Rgraphviz)
library(corpustools)
library(readtext)
library(topicmodels)
library(spacyr)
# Load text file (entire book)
lines <- read_lines("DrJekyllAndMrHyde.txt", skip = 0, n_max = -1L)
# Function to convert text to sentences
convert_text_to_sentences <- function(text, lang = "en") {
# Function to compute sentence annotations
sentence_token_annotator <- Maxent_Sent_Token_Annotator(language = lang)
# Convert text to class String from package NLP
text <- as.String(text)
# Sentence boundaries in text
sentence.boundaries <- annotate(text, sentence_token_annotator)
# Extract sentences
sentences <- text[sentence.boundaries]
# return sentences
return(sentences)
}
# Function to count number of words
nwords <- function(string, pseudo=F){
ifelse( pseudo,
pattern <- "\\S+",
pattern <- "[[:alpha:]]+"
)
str_count(string, pattern)
}
# Convert book text to sentence using function above
sentences <- convert_text_to_sentences(lines)
# Sort array by number of characters
sentences$numChar <- nchar(sentences)
sortedSentences <- sentences[order(-sentences$numChar)]
# Find 10 longest sentences
tenLongest <- sortedSentences[1:10]
tenLongest <- tenLongest[order(nwords(tenLongest))]
tenLongest
# Find number of words in each sentence
for(i in 1:10){
print(nwords(tenLongest[i]))
sprintf("\n")
}
# Extract book from corpus
intro <- bookCorpus[1]
chapter1 <- bookCorpus[2]
chapter2 <- bookCorpus[3]
# Create VCorpus of book with chapters separated out
bookCorpus <- VCorpus(DirSource("text/",ignore.case = TRUE,mode="text"))
bookCorpus
# Inspect Chapter corpus
inspect(bookCorpus)
# Check structure of chapter corpus
str(bookCorpus)
# Extract book from corpus
intro <- bookCorpus[1]
chapter1 <- bookCorpus[2]
chapter2 <- bookCorpus[3]
chapter3 <- bookCorpus[4]
chapter4 <- bookCorpus[5]
chapter5 <- bookCorpus[6]
chapter6 <- bookCorpus[7]
chapter7 <- bookCorpus[8]
chapter8 <- bookCorpus[9]
chapter9 <- bookCorpus[10]
chapter10 <- bookCorpus[11]
end <- bookCorpus[12]
# Extract book from corpus
intro <- bookCorpus[1]
# Extract book from corpus
intro <- bookCorpus[1]
# Extract book from corpus
intro <- bookCorpus[1]
# Extract book from corpus
intro <- bookCorpus[1]
# Extract book from corpus
intro <- bookCorpus[1]
chapter1 <- bookCorpus[2]
chapter2 <- bookCorpus[3]
chapter3 <- bookCorpus[4]
chapter4 <- bookCorpus[5]
chapter5 <- bookCorpus[6]
chapter6 <- bookCorpus[7]
chapter7 <- bookCorpus[8]
chapter8 <- bookCorpus[9]
chapter9 <- bookCorpus[10]
chapter10 <- bookCorpus[11]
end <- bookCorpus[12]
# Create document term matrix for bookCorpus (rows are documents, terms are words)
bookDTM <- DocumentTermMatrix(bookCorpus)
# Shows non sparse entries, sparsity, maximum term length etc.
# 12 documents, 6385 terms, Non-/sparse entries: 10488/66132
# Sparsity: 86 %
# Maximal Term Length: 30
# Weighting: term frequency(tf)
bookDTM
# Create document term matrix for bookCorpus (rows are documents, terms are words)
bookDTM <- DocumentTermMatrix(bookCorpus)
# Shows non sparse entries, sparsity, maximum term length etc.
# 12 documents, 6385 terms, Non-/sparse entries: 10488/66132
# Sparsity: 86 %
# Maximal Term Length: 30
# Weighting: term frequency(tf)
bookDTM
bookDTM$dimnames
bookDTM$dimnames$Terms
bookDTM$dimnames$Docs
# Create term document matrix for bookCorpus (rows are words, columns are documents)
bookTDM <- TermDocumentMatrix(bookCorpus)
# Shows same details as bookDTM
bookTDM
bookTDM$dimnames
bookTDM$dimnames$Terms[1:1000]
bookTDM$dimnames$Docs[1:12]
# Shows common words from each file in book corpus term document matrix, words are mostly fillers tho
inspect(bookTDM)
# Term frequency for everything
bookTF <- termFreq(bookTDM$dimnames$Terms)
bookTF
# Convert to data frame
bookDF <- as.data.frame(bookTF)
bookDF
# Start clean up of corpus
# Convert corpus to lower case
cleanCorpus <- tm_map(bookCorpus,content_transformer(tolower))
# Function to remove punctuation and numbers
removeNumPunct <- function(x) gsub("[^[:alpha:][:space:]]*","",x)
# Remove numbers and punctuation
cleanCorpus <- tm_map(cleanCorpus,content_transformer(removeNumPunct))
# toSpace function
toSpace <- content_transformer(function (x,pattern) gsub(pattern,"",x))
# Remove "/" symbols
cleanCorpus <- tm_map(cleanCorpus, toSpace, "/")
# Remove "@" symbols
cleanCorpus <- tm_map(cleanCorpus, toSpace, "@")
# Remove "|" symbols
cleanCorpus <- tm_map(cleanCorpus, toSpace, "\\|")
# Define stopwords (can bind additional stopwords to this)
myStopWords <- c(stopwords('english'))
# Remove stopwords
cleanCorpus <- tm_map(cleanCorpus,removeWords,c(stopwords('english')))
# (Removing stopwords & punctuation yields a set of documents with content bearing words)
# Compute the TDM without the stopwords
cleanTDM <- TermDocumentMatrix(cleanCorpus)
# Observe the # of entries, the sparsity and maximal term length
cleanTDM
cleanTDM2 <- TermDocumentMatrix(cleanCorpus, control = list(wordlengths=c(1,Inf)))
cleanTDM2
# Finding Frequent Words
# Find terms with a frequency of 4 or more
freq4Plus <- findFreqTerms(cleanTDM2,lowfreq=4)
freq4Plus
