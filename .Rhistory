cleanCorpus <- tm_map(cleanCorpus,content_transformer(removeNumPunct))
# toSpace function
toSpace <- content_transformer(function (x,pattern) gsub(pattern,"",x))
# Remove "/" symbols
cleanCorpus <- tm_map(cleanCorpus, toSpace, "/")
# Remove "@" symbols
cleanCorpus <- tm_map(cleanCorpus, toSpace, "@")
# Remove "|" symbols
cleanCorpus <- tm_map(cleanCorpus, toSpace, "\\|")
# Define stopwords (can bind additional stopwords to this)
myStopWords <- c(stopwords('english'))
# Remove stopwords
cleanCorpus <- tm_map(cleanCorpus,removeWords,c(stopwords('english')))
# (Removing stopwords & punctuation yields a set of documents with content bearing words)
# Compute the TDM without the stopwords
cleanTDM <- TermDocumentMatrix(cleanCorpus)
# Observe the # of entries, the sparsity and maximal term length
cleanTDM
cleanTDM2 <- TermDocumentMatrix(cleanCorpus, control = list(wordlengths=c(1,Inf)))
cleanTDM2
# Finding Frequent Words
# Find terms with a frequency of 4 or more
freq4Plus <- findFreqTerms(cleanTDM2,lowfreq=4)
freq4Plus
# Find associations
# Test
statesAssoc <- findAssocs(cleanTDM2,"states",0.5)
statesAssoc
# Find terms with frequency of 80 or more
freq80Plus <- findFreqTerms(cleanTDM2,lowfreq=80)
freq80Plus
# 5 most common terms: "hyde","jekyll","project","said","utterson"
# Associations with 5 most common terms
findAssocs(cleanTDM2,"hyde",0.5)
findAssocs(cleanTDM2,"jekyll",0.5)
findAssocs(cleanTDM2,"project",0.5)
findAssocs(cleanTDM2,"said",0.5)
findAssocs(cleanTDM2,"utterson",0.5)
# Term Frequency
termFreq <- rowSums(as.matrix(cleanTDM2))
termFreqSub <- subset(termFreq,termFreq >=6)
termFreqdf <- as.data.frame(names(termFreq),freq=termFreq)
termFreq
# Binary/Boolean Term Frequency check
termFreq("word",cleanTDM2)
# Remove Sparse Terms from cleanTDM2
sparseTDM <- removeSparseTerms(cleanTDM2,sparse=0.95)
inspect(sparseTDM)
sparseTDM
# Finding Informative Words
inspect(chapter1)
# Word Cloud
m1 <- as.matrix(cleanTDM2)
word.freq <- sort(rowSums(m1),decreasing = T)
word.freq
pal <- brewer.pal(9,"BuGn")
# Word Cloud
m1 <- as.matrix(cleanTDM2)
# Word Cloud
m1 <- as.matrix(cleanTDM2)
word.freq <- sort(rowSums(m1),decreasing = T)
word.freq
pal <- brewer.pal(9,"BuGn")
pal <- pal[-(1:4)]
wordcloud(words = names(word.freq),freq=word.freq,min.freq=3,random.order=F,colors=pal)
# Frequency Analysis - I
cleanDTM <- DocumentTermMatrix(cleanCorpus)
freq <- colSums(as.matrix(cleanDTM))
cleanDTM
length(freq) # total number of terms
# Sort freq in descending order
ord <- order(freq,decreasing=TRUE)
# Most frequently occuring terms
freq[head(ord)]
# Least frequently occuring terms
freq[tail(ord)]
# Reducing data: word lengths between 4 and 20
cleanDTMR <- DocumentTermMatrix(cleanCorpus,control=list(wordLengths=c(4,20)))
cleanDTMR
freqr <- colSums(as.matrix(cleanDTMR))
ordr <- order(freqr, decreasing = TRUE)
freqr[head(ordr)]
freqr[tail(ordr)]
clBegin <- bookCorpus$content[1][[1]][["content"]]
clChapter1 <- bookCorpus$content[2][[1]][["content"]]
clChapter2 <- bookCorpus$content[3][[1]][["content"]]
clChapter3 <- bookCorpus$content[4][[1]][["content"]]
clChapter4 <- bookCorpus$content[5][[1]][["content"]]
clChapter5 <- bookCorpus$content[6][[1]][["content"]]
clChapter6 <- bookCorpus$content[7][[1]][["content"]]
clChapter7 <- bookCorpus$content[8][[1]][["content"]]
clChapter8 <- bookCorpus$content[9][[1]][["content"]]
clChapter9 <- bookCorpus$content[10][[1]][["content"]]
clChapter10 <- bookCorpus$content[11][[1]][["content"]]
clEnd <- bookCorpus$content[12][[1]][["content"]]
# get distances
# Chapter 1
x1 <- VCorpus(DirSource("text/chapter1/",ignore.case = TRUE,mode="text"))
x1TDM <- TermDocumentMatrix(x1)
x1TDM
ch1Dist <- dist(x1TDM)
# Chapter 2
x2 <- VCorpus(DirSource("text/chapter2/",ignore.case = TRUE,mode="text"))
x2TDM <- TermDocumentMatrix(x2)
x2TDM
# Beginning / Intro before ch1
clIntroSent <- syuzhet::get_nrc_sentiment(clBegin)
# Beginning / Intro before ch1
clIntroSent <- syuzhet::get_nrc_sentiment(clBegin)
clIntroSent
head(clIntroSent)
# Row sum of sentiments
introRowSum <- rowSums(clIntroSent)
introRowSum
# Column sum of sentiments
introColSum <- colSums(clIntroSent)
introColSum
# Chapter 1
clChapter1Sent <- syuzhet::get_nrc_sentiment(clChapter1)
clChapter1Sent
head(clChapter1Sent)
# Row sum of sentiments
chapter1RowSum <- rowSums(clChapter1Sent)
chapter1RowSum
# Column sum of sentiments
chapter1ColSum <- colSums(clChapter1Sent)
chapter1ColSum
# Chapter 2
clChapter2Sent <- syuzhet::get_nrc_sentiment(clChapter2)
clChapter2Sent
head(clChapter2Sent)
# Row sum of sentiments
chapter2RowSum <- rowSums(clChapter2Sent)
chapter2RowSum
# Column sum of sentiments
chapter2ColSum <- colSums(clChapter2Sent)
chapter2ColSum
# Chapter 3
clChapter3Sent <- syuzhet::get_nrc_sentiment(clChapter3)
clChapter3Sent
head(clChapter3Sent)
# Row sum of sentiments
chapter3RowSum <- rowSums(clChapter3Sent)
chapter3RowSum
# Column sum of sentiments
chapter3ColSum <- colSums(clChapter3Sent)
chapter3ColSum
# Chapter 4
clChapter4Sent <- syuzhet::get_nrc_sentiment(clChapter4)
clChapter4Sent
head(clChapter4Sent)
# Row sum of sentiments
chapter4RowSum <- rowSums(clChapter4Sent)
chapter4RowSum
# Column sum of sentiments
chapter4ColSum <- colSums(clChapter4Sent)
chapter4ColSum
# Chapter 5
clChapter5Sent <- syuzhet::get_nrc_sentiment(clChapter5)
clChapter5Sent
head(clChapter5Sent)
# Row sum of sentiments
chapter5RowSum <- rowSums(clChapter5Sent)
chapter5RowSum
# Column sum of sentiments
chapter5ColSum <- colSums(clChapter5Sent)
chapter5ColSum
# Chapter 6
clChapter6Sent <- syuzhet::get_nrc_sentiment(clChapter6)
clChapter6Sent
head(clChapter6Sent)
# Row sum of sentiments
chapter6RowSum <- rowSums(clChapter6Sent)
chapter6RowSum
# Column sum of sentiments
chapter6ColSum <- colSums(clChapter6Sent)
chapter6ColSum
# Chapter 7
clChapter7Sent <- syuzhet::get_nrc_sentiment(clChapter7)
clChapter7Sent
head(clChapter7Sent)
# Row sum of sentiments
chapter7RowSum <- rowSums(clChapter7Sent)
chapter7RowSum
# Column sum of sentiments
chapter7ColSum <- colSums(clChapter7Sent)
chapter7ColSum
# Chapter 8
clChapter8Sent <- syuzhet::get_nrc_sentiment(clChapter8)
clChapter8Sent
head(clChapter8Sent)
# Row sum of sentiments
chapter8RowSum <- rowSums(clChapter8Sent)
chapter8RowSum
# Column sum of sentiments
chapter8ColSum <- colSums(clChapter8Sent)
chapter8ColSum
# Chapter 9
clChapter9Sent <- syuzhet::get_nrc_sentiment(clChapter9)
clChapter9Sent
head(clChapter9Sent)
# Row sum of sentiments
chapter9RowSum <- rowSums(clChapter9Sent)
chapter9RowSum
# Column sum of sentiments
chapter9ColSum <- colSums(clChapter9Sent)
chapter9ColSum
# Chapter 10
clChapter10Sent <- syuzhet::get_nrc_sentiment(clChapter10)
clChapter10Sent
head(clChapter10Sent)
# Row sum of sentiments
chapter10RowSum <- rowSums(clChapter10Sent)
chapter10RowSum
# Column sum of sentiments
chapter10ColSum <- colSums(clChapter10Sent)
chapter10ColSum
# Summary of book.spc
summary(book.spc)
# use text2spc.fnc() from zipFR
table(table(wordDF$words))
names(wordDF$words)
txt <- as.String(wordDF$Words)
txt
txt <- na.rm(txt)
txt <- na.omit(txt)
txt
names(txt)
txt <- stri_extract_all_words(txt, simplify = TRUE)
txt
txt[1]
book.spc <- text2spc.fnc(txt)
book.spc
# Summary of book.spc
summary(book.spc)
# Sample Size, Vocabulary and Hapax Counts
N(book.spc)
V(book.spc)
Vm(book.spc,1)
# use text2spc.fnc() from zipFR
table(table(wordDF$words))
txt <- as.String(wordDF$Words)
txt <- na.omit(txt)
txt <- stri_extract_all_words(txt, simplify = TRUE)
book.spc <- text2spc.fnc(txt)
# use text2spc.fnc() from zipFR
txt <- as.String(wordDF$Words)
txt <- na.omit(txt)
txt <- stri_extract_all_words(txt, simplify = TRUE)
book.spc <- text2spc.fnc(txt)
book.spc
# Summary of book.spc
summary(book.spc)
# Sample Size, Vocabulary and Hapax Counts
N(book.spc)
V(book.spc)
Vm(book.spc,1)
# Plot Word Frequency Spectrum on a logarithm scale with respect to x
plot(book.spc,log="x")
# Get all words whose length is greater than 6 characters from chapter 1
# Get VCorpus of chapter 1
ch1Corpus <- VCorpus(DirSource("text/chapter1/",ignore.case = TRUE,mode="text"))
txt <- text(txt)
txt
# use text2spc.fnc() from zipFR
txt <- as.String(wordDF$Words)
txt <- na.omit(txt)
txt <- stri_extract_all_words(txt, simplify = TRUE)
txt
str(txt)
names(txt)
names(txt) <- txt
txt
names(txt)
names(txt)
names(txt)
book.spc <- text2spc.fnc(txt)
book.spc
# Summary of book.spc
summary(book.spc)
# Sample Size, Vocabulary and Hapax Counts
N(book.spc)
V(book.spc)
Vm(book.spc,1)
# Plot Word Frequency Spectrum on a logarithm scale with respect to x
plot(book.spc,log="x")
# Get all words whose length is greater than 6 characters from chapter 1
# Get VCorpus of chapter 1
ch1Corpus <- VCorpus(DirSource("text/chapter1/",ignore.case = TRUE,mode="text"))
ch1Corpus <- tm_map(ch1Corpus,toSpace,"â€™")
# Summary of book.spc
summary(book.spc)
# Sample Size, Vocabulary and Hapax Counts
N(book.spc)
V(book.spc)
Vm(book.spc,1)
# Plot Word Frequency Spectrum on a logarithm scale with respect to x
plot(book.spc,log="x")
# use text2spc.fnc() from zipFR
txt <- as.String(wordDF$Words)
txt <- na.omit(txt)
txt <- stri_extract_all_words(txt, simplify = TRUE)
str(txt)
names(txt) <- txt
names(txt)
book.spc <- text2spc.fnc(txt)
book.spc
# Summary of book.spc
summary(book.spc)
# Sample Size, Vocabulary and Hapax Counts
N(book.spc)
freq(txt)
wordFreq(txt)
txt$Freq
txt <- as.data.frame(txt)
txt
# use text2spc.fnc() from zipFR
txt <- as.String(wordDF$Words)
txt <- na.omit(txt)
txt <- stri_extract_all_words(txt, simplify = TRUE)
str(txt)
names(txt) <- txt
names(txt)
txt <- as.data.frame(txt)
txt
txt[1]
book.spc <- text2spc.fnc(txt)
book.spc
# Summary of book.spc
summary(book.spc)
# Sample Size, Vocabulary and Hapax Counts
N(book.spc)
V(book.spc)
Vm(book.spc,1)
# Plot Word Frequency Spectrum on a logarithm scale with respect to x
plot(book.spc,log="x")
# Get all words whose length is greater than 6 characters from chapter 1
# Get VCorpus of chapter 1
ch1Corpus <- VCorpus(DirSource("text/chapter1/",ignore.case = TRUE,mode="text"))
# Convert to data frame
words <- as.data.frame(cleanTDM$dimnames$Terms)
freq <- as.data.frame(cleanTF)
length(words)
nrow(freq)
nrow(words)
words[1:4280,1]
freq[1:4280,1]
wordDF <- data.frame("Words" = words[1:4280,1], "Freq" = freq[1:4280,1], stringsAsFactors = FALSE)
# Convert to character vector
wordDF$Words <- as.character(wordDF$Words)
is.character(wordDF$Words)
str(wordDF)
str(wordDF$Words)
# use text2spc.fnc() from zipFR
txt <- as.String(wordDF$Words)
txt <- na.omit(txt)
txt <- stri_extract_all_words(txt, simplify = TRUE)
str(txt)
names(txt) <- txt
names(txt)
txt <- as.data.frame(txt)
txt[1]
book.spc <- text2spc.fnc(wordDF)
book.spc
# Summary of book.spc
summary(book.spc)
# Sample Size, Vocabulary and Hapax Counts
N(book.spc)
V(book.spc)
Vm(book.spc,1)
# Plot Word Frequency Spectrum on a logarithm scale with respect to x
plot(book.spc,log="x")
# Plot Word Frequency Spectrum on a logarithm scale with respect to x
plot(book.spc)
plot(book.spc,log="y")
plot(book.spc,log="x")
# Import data document using read_lines
dataDoc <- toString(read_lines("DrJekyllAndMrHyde.txt", skip = 0, n_max = -1L))
rt <- readtext("text/*.txt",docvarsfrom="filepaths")
rt
# Stringi
# Transform to lower case
dataDoc <- stri_trans_tolower(dataDoc)
# Remove tags
dataDoc <- stri_replace_all(dataDoc,"",regex = "<.*?>")
# Strip surrounding white space
dataDoc <- str_trim(dataDoc)
# Reduce repeated whitespace inside string
dataDoc <- str_squish(dataDoc)
dataDoc
# Quanteda
# Tokenization
toks <- quanteda::tokens(dataDoc)
# Convert to lower case
toks <- tokens_tolower(toks)
# Remove stopwords
sw <- stopwords("english")
head(sw)
tokens_remove(toks,sw)
fulltext <- corpus(rt)
dtm <- dfm(fulltext, tolower = TRUE, stem = TRUE,
remove_punct = TRUE,remove = stopwords("english"))
dtm@docvars
dtm@Dimnames
# Document Frequency
doc_freq <- docfreq(dtm)
doc_freq
# Terms with frequency >= 6
dtm6 <- dtm[, doc_freq >= 6]
# TF-IDF
dtm1 <- dfm_weight(dtm, "tfidf")
dtm6
dtm6@weightTf
dtm6@factors
dtm6@settings
# Terms with frequency >= 6
dtm6 <- dtm[, doc_freq >= 6]
doc_freq
plot(doc_req)
plot(doc_freq)
hist(doc_freq)
# TF-IDF
dtm1 <- dfm_weight(dtm, "tfidf")
dfm_tfidf(dtm)
# textreuse
# Function 1
# Biggram tokenizer
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
bigramTDM <- TermDocumentMatrix(ch1Corpus, control = list(tokenize = BigramTokenizer, wordlengths=c(6,Inf)))
library(dplyr)
# Tidy text
text_df <- tibble(line = 1:4, text = dataDoc)
text_def
text_df
dataDoc
# Tidy text
text_df <- tibble(line = 10:20, text = dataDoc)
text_df
# Tidy text
text_df <- tibble(line = c(1,2,3,4), text = dataDoc)
text_df
# Convert text to sentences
dataDoc <- convert_text_to_sentences(dataDoc)
dataDoc
# Tidy text
text_df <- tibble(line = 10:20, text = dataDoc)
# Tidy text
text_df <- tibble(line = 1, text = dataDoc)
text_df
# Tidy text
text_df <- tibble(line = 1:4, text = dataDoc)
text_df
help(tidytext)
install.packages("tidytext")
help(tidytext)
help(hunspell)
??tidytext
# Get sentiments
get_sentiments(dataDoc)
library(dplyr)
library(broom)
library(tidyr)
library(ggplot2)
# Get sentiments
get_sentiments(dataDoc)
dtm@docvars
dtm@Dimnames
# Extract words
x <- dtm@Dimnames
x
get_nrc_sentiment(x)
x <- as.vector(x)
x
is.character(x)
x <- as.character(x)
x
# Extract words
x <- dtm@Dimnames
x
x <- as.data.frame.character(x, stringsAsFactors=F)
is.character(x)
x
x[1]
# Extract words
x <- dtm@Dimnames
# Extract words
x <- dtm@Dimnames
# Extract words
wrds <- dtm@Dimnames
text_df <- tibble(line = 1:4, text = wrds)
text_df
text_df <- tibble(line = 1, text = wrds)
text_df
text_df <- tibble(line = 1:2, text = wrds)
text_df
text_df <- tibble(line = 1:3, text = wrds)
wrds
str(wrds)
wrds$features
str(wrds$features)
get_nrc_values(wrds$features)
get_nrc_sentiment(wrds$features)
# Get nrc values
get_nrc_values(wrds$features)
# Explore Corpus Term Frequency Characteristics
Zipf_plot(cleanCorpus)
# Explore Corpus Term Frequency Characteristics
Zipf_plot(cleanTDM)
# plot words
plot_words(cleanTDM)
library(igraph)
igraph(wrds)
