bookDTM$dimnames
bookDTM$dimnames$Terms
bookDTM$dimnames$Docs
# Create term document matrix for bookCorpus (rows are words, columns are documents)
bookTDM <- TermDocumentMatrix(bookCorpus)
# Shows same details as bookDTM
bookTDM
bookTDM$dimnames
bookTDM$dimnames$Terms[1:1000]
bookTDM$dimnames$Docs[1:12]
# Shows common words from each file in book corpus term document matrix, words are mostly fillers tho
inspect(bookTDM)
# Term frequency for everything
bookTF <- termFreq(bookTDM$dimnames$Terms)
bookTF
# Convert to data frame
bookDF <- as.data.frame(bookTF)
bookDF
# Start clean up of corpus
# Convert corpus to lower case
cleanCorpus <- tm_map(bookCorpus,content_transformer(tolower))
# Function to remove punctuation and numbers
removeNumPunct <- function(x) gsub("[^[:alpha:][:space:]]*","",x)
# Remove numbers and punctuation
cleanCorpus <- tm_map(cleanCorpus,content_transformer(removeNumPunct))
# toSpace function
toSpace <- content_transformer(function (x,pattern) gsub(pattern,"",x))
# Remove "/" symbols
cleanCorpus <- tm_map(cleanCorpus, toSpace, "/")
# Remove "@" symbols
cleanCorpus <- tm_map(cleanCorpus, toSpace, "@")
# Remove "|" symbols
cleanCorpus <- tm_map(cleanCorpus, toSpace, "\\|")
# Define stopwords (can bind additional stopwords to this)
myStopWords <- c(stopwords('english'))
# Remove stopwords
cleanCorpus <- tm_map(cleanCorpus,removeWords,c(stopwords('english')))
# (Removing stopwords & punctuation yields a set of documents with content bearing words)
# Compute the TDM without the stopwords
cleanTDM <- TermDocumentMatrix(cleanCorpus)
# Observe the # of entries, the sparsity and maximal term length
cleanTDM
cleanTDM2 <- TermDocumentMatrix(cleanCorpus, control = list(wordlengths=c(1,Inf)))
cleanTDM2
# Finding Frequent Words
# Find terms with a frequency of 4 or more
freq4Plus <- findFreqTerms(cleanTDM2,lowfreq=4)
freq4Plus
# Find associations
# Test
statesAssoc <- findAssocs(cleanTDM2,"states",0.5)
statesAssoc
# Find terms with frequency of 80 or more
freq80Plus <- findFreqTerms(cleanTDM2,lowfreq=80)
freq80Plus
# 5 most common terms: "hyde","jekyll","project","said","utterson"
# Associations with 5 most common terms
findAssocs(cleanTDM2,"hyde",0.5)
findAssocs(cleanTDM2,"jekyll",0.5)
findAssocs(cleanTDM2,"project",0.5)
findAssocs(cleanTDM2,"said",0.5)
findAssocs(cleanTDM2,"utterson",0.5)
# Term Frequency
termFreq <- rowSums(as.matrix(cleanTDM2))
termFreqSub <- subset(termFreq,termFreq >=6)
termFreqdf <- as.data.frame(names(termFreq),freq=termFreq)
termFreq
# Binary/Boolean Term Frequency check
termFreq("word",cleanTDM2)
# Remove Sparse Terms from cleanTDM2
sparseTDM <- removeSparseTerms(cleanTDM2,sparse=0.95)
inspect(sparseTDM)
sparseTDM
# Finding Informative Words
inspect(chapter1)
# Word Cloud
m1 <- as.matrix(cleanTDM2)
word.freq <- sort(rowSums(m1),decreasing = T)
word.freq
pal <- brewer.pal(9,"BuGn")
pal <- pal[-(1:4)]
wordcloud(words = names(word.freq),freq=word.freq,min.freq=3,random.order=F,colors=pal)
# Frequency Analysis - I
cleanDTM <- DocumentTermMatrix(cleanCorpus)
freq <- colSums(as.matrix(cleanDTM))
cleanDTM
length(freq) # total number of terms
# Sort freq in descending order
ord <- order(freq,decreasing=TRUE)
# Most frequently occuring terms
freq[head(ord)]
# Least frequently occuring terms
freq[tail(ord)]
# Reducing data: word lengths between 4 and 20
cleanDTMR <- DocumentTermMatrix(cleanCorpus,control=list(wordLengths=c(4,20)))
cleanDTMR
freqr <- colSums(as.matrix(cleanDTMR))
ordr <- order(freqr, decreasing = TRUE)
freqr[head(ordr)]
freqr[tail(ordr)]
clBegin <- bookCorpus$content[1][[1]][["content"]]
clChapter1 <- bookCorpus$content[2][[1]][["content"]]
clChapter2 <- bookCorpus$content[3][[1]][["content"]]
clChapter3 <- bookCorpus$content[4][[1]][["content"]]
clChapter4 <- bookCorpus$content[5][[1]][["content"]]
clChapter5 <- bookCorpus$content[6][[1]][["content"]]
clChapter6 <- bookCorpus$content[7][[1]][["content"]]
clChapter7 <- bookCorpus$content[8][[1]][["content"]]
clChapter8 <- bookCorpus$content[9][[1]][["content"]]
clChapter9 <- bookCorpus$content[10][[1]][["content"]]
clChapter10 <- bookCorpus$content[11][[1]][["content"]]
clEnd <- bookCorpus$content[12][[1]][["content"]]
clChapter1 <- na.omit(clChapter1)
clChapter1 <- na.omit(clChapter1)
clChapter1 <- na.omit(clChapter1)
clChapter2 <- na.omit(clChapter2)
clChapter3 <- na.omit(clChapter3)
clChapter4 <- na.omit(clChapter4)
clChapter5 <- na.omit(clChapter5)
clChapter6 <- na.omit(clChapter6)
clChapter7 <- na.omit(clChapter7)
clChapter8 <- na.omit(clChapter8)
clChapter9 <- na.omit(clChapter9)
clChapter10 <- na.omit(clChapter10)
ch1Dist <- dist(clChapter1)
ch1Dist <- dist(clChapter1)
ch2Dist <- dist(clChapter2)
ch3Dist <- dist(clChapter3)
ch4Dist <- dist(clChapter4)
ch5Dist <- dist(clChapter5)
ch6Dist <- dist(clChapter6)
ch7Dist <- dist(clChapter7)
ch8Dist <- dist(clChapter8)
ch9Dist <- dist(clChapter9)
ch10Dist <- dist(clChapter10)
# Chapter 1
fitCH1 <- hclust(ch1Dist,method="ward.D2")
# Chapter 1
fitCH1 <- hclust(ch1Dist,method="ward.D2")
# Chapter 2
fitCH2 <- hclust(distMatrix,method="ward.D2")
# Chapter 1
fitCH1 <- hclust(ch1Dist,method="ward.D2")
fitCH1
# Chapter 1
fitCH1 <- hclust(ch1Dist,method="ward.D2")
# Chapter 1
fitCH1 <- hclust(ch1Dist,method="euclidean")
fitCH1
# Chapter 1
fitCH1 <- hclust(ch1Dist,method="average")
fitCH1
# Chapter 1
fitCH1 <- hclust(ch1Dist,method="average")
ch1Dist
ch1Dist <- na.omit(ch1Dist)
ch1Dist
# Chapter 1
fitCH1 <- hclust(ch1Dist,method="average")
# Chapter 1
fitCH1 <- hclust(ch1Dist,method="ward.D2")
# Chapter 1
fitCH1 <- hclust(ch1Dist[1:6000],method="ward.D2")
fitCH1
# Chapter 1
fitCH1 <- hclust(ch1Dist[1:6000],method="ward.D2")
# Chapter 1
fitCH1 <- hclust(ch1Dist[1:10],method="ward.D2")
fitCH1
# Chapter 1
fitCH1 <- hclust(ch1Dist[1:10],method="ward.D2")
ch1Dist <- as.numeric(ch1Dist)
ch1Dist <- na.omit(ch1Dist)
# Chapter 1
fitCH1 <- hclust(ch1Dist[1:10],method="ward.D2")
fitCH1
library(cluster)
DM = as.matrix(dist(ruspini))
HC = hclust(as.dist(DM), method="single")
HC
plot(HC)
DM = as.matrix(dist(ch1Dist))
HC = hclust(as.dist(DM), method="single")
HC
DM = as.matrix(dist(clChapter1))
HC = hclust(as.dist(DM), method="single")
HC
ch1 <- TermDocumentMatrix(clChapter1)
ch1 <- clChapter1
ch1 <- bookCorpus[1]$Begin.txt
ch1
str(bookCorpus$Begin.txt)
bookCorpus
bookCorpus
bookCorpus[1]
bookCorpus$content
bookCorpus$content[1]
bookCorpus$content[1][1]
bookCorpus$content[1][1]["content"]
bookCorpus$content[1][1]["content"]]
bookCorpus$content[1][1][["content"]]
bookCorpus$content[1][[1]][["content"]]
bookCorpus$content[1][[1]][["content"]]
test1 <- as.data.frame(bookCorpus$content[1][[1]][["content"]])
test1 <- as.numeric(test1)
test1 <- as.numeric(as.character(test1))
test1
test1
x1 <- VCorpus(DirSource("text/Chapter1/Chapter01.txt",ignore.case = TRUE,mode="text"))
setwd("~/Desktop/DataScience/BigData/Projects/Project3BigData")
x1 <- VCorpus(DirSource("text/Chapter1/Chapter01.txt",ignore.case = TRUE,mode="text"))
x1 <- VCorpus(DirSource("text/chapter1/Chapter01.txt",ignore.case = TRUE,mode="text"))
setwd("~/Desktop/DataScience/BigData/Projects/Project3BigData")
x1 <- VCorpus(DirSource("text/chapter1/Chapter01.txt",ignore.case = TRUE,mode="text"))
x1 <- VCorpus(DirSource("text/chapter1/",ignore.case = TRUE,mode="text"))
DocumentTermMatrix(x1)
x1DTM <- DocumentTermMatrix(x1)
x1m <- as.matrix(x1DTM)
dist(x1m)
dist(x1DTM)
x1DTM <- TermDocumentMatrix(x1)
x1TDM <- TermDocumentMatrix(x1)
x1TDM
X1TDM
X1TDM
x1TDM
dist(x1TDM)
ch1Dist <- dist(x1TDM)
# Chapter 1
fitCH1 <- hclust(ch1Dist,method="ward.D2")
fitCH1
plot(CH1)
plot(fitCH1)
ch1Dist <- dist(x1TDM)
# Chapter 1
fitCH1 <- hclust(ch1Dist,method="ward.D2")
# get distances
# Chapter 1
x1 <- VCorpus(DirSource("text/chapter1/",ignore.case = TRUE,mode="text"))
x1TDM <- TermDocumentMatrix(x1)
x1TDM
ch1Dist <- dist(x1TDM)
# Chapter 2
x2 <- VCorpus(DirSource("text/chapter2/",ignore.case = TRUE,mode="text"))
x2TDM <- TermDocumentMatrix(x2)
x2TDM
ch1Dist <- dist(x1TDM)
# Chapter 3
x3 <- VCorpus(DirSource("text/chapter3/",ignore.case = TRUE,mode="text"))
x3TDM <- TermDocumentMatrix(x3)
x3TDM
ch3Dist <- dist(x3TDM)
# get distances
# Chapter 1
x1 <- VCorpus(DirSource("text/chapter1/",ignore.case = TRUE,mode="text"))
x1TDM <- TermDocumentMatrix(x1)
x1TDM
ch1Dist <- dist(x1TDM)
# Chapter 2
x2 <- VCorpus(DirSource("text/chapter2/",ignore.case = TRUE,mode="text"))
x2TDM <- TermDocumentMatrix(x2)
x2TDM
ch1Dist <- dist(x1TDM)
# Chapter 3
x3 <- VCorpus(DirSource("text/chapter3/",ignore.case = TRUE,mode="text"))
x3TDM <- TermDocumentMatrix(x3)
x3TDM
ch3Dist <- dist(x3TDM)
# Chapter 4
x4 <- VCorpus(DirSource("text/chapter4/",ignore.case = TRUE,mode="text"))
x4TDM <- TermDocumentMatrix(x4)
x4TDM
ch4Dist <- dist(x4TDM)
# Chapter 5
x5 <- VCorpus(DirSource("text/chapter5/",ignore.case = TRUE,mode="text"))
x5TDM <- TermDocumentMatrix(x5)
x5TDM
ch5Dist <- dist(x5TDM)
# Chapter 6
x6 <- VCorpus(DirSource("text/chapter6/",ignore.case = TRUE,mode="text"))
x6TDM <- TermDocumentMatrix(x6)
x6TDM
ch6Dist <- dist(x6TDM)
# Chapter 7
x7 <- VCorpus(DirSource("text/chapter7/",ignore.case = TRUE,mode="text"))
x7TDM <- TermDocumentMatrix(x7)
x7TDM
ch7Dist <- dist(x7TDM)
# Chapter 8
x8 <- VCorpus(DirSource("text/chapter8/",ignore.case = TRUE,mode="text"))
x8TDM <- TermDocumentMatrix(x8)
x8TDM
ch8Dist <- dist(x8TDM)
# Chapter 9
x9 <- VCorpus(DirSource("text/chapter9/",ignore.case = TRUE,mode="text"))
x9TDM <- TermDocumentMatrix(x9)
x9TDM
ch9Dist <- dist(x9TDM)
# Chapter 10
x10 <- VCorpus(DirSource("text/chapter10/",ignore.case = TRUE,mode="text"))
x10TDM <- TermDocumentMatrix(x10)
x10TDM
ch10Dist <- dist(x10TDM)
# Chapter 1
fitCH1 <- hclust(ch1Dist,method="ward.D2")
fitCH1
plot(fitCH1)
# Chapter 2
fitCH2 <- hclust(ch2Dist,method="ward.D2")
# get distances
# Chapter 1
x1 <- VCorpus(DirSource("text/chapter1/",ignore.case = TRUE,mode="text"))
x1TDM <- TermDocumentMatrix(x1)
x1TDM
ch1Dist <- dist(x1TDM)
# Chapter 2
x2 <- VCorpus(DirSource("text/chapter2/",ignore.case = TRUE,mode="text"))
x2TDM <- TermDocumentMatrix(x2)
x2TDM
ch2Dist <- dist(x2TDM)
# Chapter 3
x3 <- VCorpus(DirSource("text/chapter3/",ignore.case = TRUE,mode="text"))
x3TDM <- TermDocumentMatrix(x3)
x3TDM
ch3Dist <- dist(x3TDM)
# Chapter 4
x4 <- VCorpus(DirSource("text/chapter4/",ignore.case = TRUE,mode="text"))
x4TDM <- TermDocumentMatrix(x4)
x4TDM
ch4Dist <- dist(x4TDM)
# Chapter 5
x5 <- VCorpus(DirSource("text/chapter5/",ignore.case = TRUE,mode="text"))
x5TDM <- TermDocumentMatrix(x5)
x5TDM
ch5Dist <- dist(x5TDM)
# Chapter 6
x6 <- VCorpus(DirSource("text/chapter6/",ignore.case = TRUE,mode="text"))
x6TDM <- TermDocumentMatrix(x6)
x6TDM
ch6Dist <- dist(x6TDM)
# Chapter 7
x7 <- VCorpus(DirSource("text/chapter7/",ignore.case = TRUE,mode="text"))
x7TDM <- TermDocumentMatrix(x7)
x7TDM
ch7Dist <- dist(x7TDM)
# Chapter 8
x8 <- VCorpus(DirSource("text/chapter8/",ignore.case = TRUE,mode="text"))
x8TDM <- TermDocumentMatrix(x8)
x8TDM
ch8Dist <- dist(x8TDM)
# Chapter 9
x9 <- VCorpus(DirSource("text/chapter9/",ignore.case = TRUE,mode="text"))
x9TDM <- TermDocumentMatrix(x9)
x9TDM
ch9Dist <- dist(x9TDM)
# Chapter 10
x10 <- VCorpus(DirSource("text/chapter10/",ignore.case = TRUE,mode="text"))
x10TDM <- TermDocumentMatrix(x10)
x10TDM
ch10Dist <- dist(x10TDM)
# Chapter 1
fitCH1 <- hclust(ch1Dist,method="ward.D2")
fitCH1
plot(fitCH1)
# Chapter 2
fitCH2 <- hclust(ch2Dist,method="ward.D2")
fitCH2
plot(fitCH2)
setwd("~/Desktop/DataScience/BigData/Projects/Project3BigData")
# Load required libraries
library(devtools)
library(textreuse)
library(SnowballC)
library(wordcloud)
library(NLP)
library(rJava)
library(wordnet)
library(tm)
library(zipfR)
library(textreuse)
library(quanteda)
library(stringi)
library(syuzhet)
library(corpus)
library(openNLP)
library(readr)
library(stringr)
# Load text file (entire book)
lines <- read_lines("DrJekyllAndMrHyde.txt", skip = 0, n_max = -1L)
lines
# Check numrows
nrow(lines)
# Check head
head(lines)
# Create VCorpus of book with chapters separated out
bookCorpus <- VCorpus(DirSource("text/",ignore.case = TRUE,mode="text"))
bookCorpus
# Inspect Chapter corpus
inspect(bookCorpus)
# Check structure of chapter corpus
str(bookCorpus)
# Function to convert text to sentences
convert_text_to_sentences <- function(text, lang = "en") {
# Function to compute sentence annotations using the Apache OpenNLP Maxent sentence detector employing the default model for language 'en'.
sentence_token_annotator <- Maxent_Sent_Token_Annotator(language = lang)
# Convert text to class String from package NLP
text <- as.String(text)
# Sentence boundaries in text
sentence.boundaries <- annotate(text, sentence_token_annotator)
# Extract sentences
sentences <- text[sentence.boundaries]
# return sentences
return(sentences)
}
sentences <- convert_text_to_sentences(lines)
str(sentences)
sentences[1]
summary(sentences)
str_length(sentences[2])
# Sort array by number of characters
sentences$numChar <- nchar(sentences)
sentences$numChar
sortedSentences <- sentences[order(-sentences$numChar)]
# Find 10 longest sentences
tenLongest <- sortedSentences[1:10]
tenLongest
tenLongest <- tenLongest[order(-nwords(tenLongest))]
# Function to count number of words
nwords <- function(string, pseudo=F){
ifelse( pseudo,
pattern <- "\\S+",
pattern <- "[[:alpha:]]+"
)
str_count(string, pattern)
}
# Find number of words in each sentence (add to report)
for(i in 1:10){
print(nwords(tenLongest[i]))
sprintf("\n")
}
# Extract book from corpus
intro <- bookCorpus[1]
chapter1 <- bookCorpus[2]
chapter2 <- bookCorpus[3]
chapter3 <- bookCorpus[4]
chapter4 <- bookCorpus[5]
chapter5 <- bookCorpus[6]
chapter6 <- bookCorpus[7]
chapter7 <- bookCorpus[8]
chapter8 <- bookCorpus[9]
chapter9 <- bookCorpus[10]
chapter10 <- bookCorpus[11]
end <- bookCorpus[12]
# Create document term matrix for bookCorpus (rows are documents, terms are words)
bookDTM <- DocumentTermMatrix(bookCorpus)
# Shows non sparse entries, sparsity, maximum term length etc.
# 12 documents, 6385 terms, Non-/sparse entries: 10488/66132
# Sparsity: 86 %
# Maximal Term Length: 30
# Weighting: term frequency(tf)
bookDTM
bookDTM$dimnames
bookDTM$dimnames$Terms
bookDTM$dimnames$Docs
# Create term document matrix for bookCorpus (rows are words, columns are documents)
bookTDM <- TermDocumentMatrix(bookCorpus)
# Shows same details as bookDTM
bookTDM
bookTDM$dimnames
bookTDM$dimnames$Terms[1:1000]
bookTDM$dimnames$Docs[1:12]
# Shows common words from each file in book corpus term document matrix, words are mostly fillers tho
inspect(bookTDM)
# Term frequency for everything
bookTF <- termFreq(bookTDM$dimnames$Terms)
bookTF
# Convert to data frame
bookDF <- as.data.frame(bookTF)
bookDF
# Start clean up of corpus
# Convert corpus to lower case
cleanCorpus <- tm_map(bookCorpus,content_transformer(tolower))
# Function to remove punctuation and numbers
removeNumPunct <- function(x) gsub("[^[:alpha:][:space:]]*","",x)
# Remove numbers and punctuation
cleanCorpus <- tm_map(cleanCorpus,content_transformer(removeNumPunct))
# toSpace function
toSpace <- content_transformer(function (x,pattern) gsub(pattern,"",x))
# Remove "/" symbols
cleanCorpus <- tm_map(cleanCorpus, toSpace, "/")
# Remove "@" symbols
cleanCorpus <- tm_map(cleanCorpus, toSpace, "@")
# Remove "|" symbols
cleanCorpus <- tm_map(cleanCorpus, toSpace, "\\|")
# Define stopwords (can bind additional stopwords to this)
myStopWords <- c(stopwords('english'))
# Remove stopwords
cleanCorpus <- tm_map(cleanCorpus,removeWords,c(stopwords('english')))
# (Removing stopwords & punctuation yields a set of documents with content bearing words)
# Compute the TDM without the stopwords
cleanTDM <- TermDocumentMatrix(cleanCorpus)
# Observe the # of entries, the sparsity and maximal term length
cleanTDM
cleanTDM2 <- TermDocumentMatrix(cleanCorpus, control = list(wordlengths=c(1,Inf)))
cleanTDM2
# Finding Frequent Words
# Find terms with a frequency of 4 or more
freq4Plus <- findFreqTerms(cleanTDM2,lowfreq=4)
freq4Plus
# Find associations
# Test
statesAssoc <- findAssocs(cleanTDM2,"states",0.5)
