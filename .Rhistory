plot(bigramTDM, terms = findFreqTerms(bigramTDM, lowfreq = 2)[1:10], corThreshold = 0.2)
ch1Corpus <- VCorpus(DirSource("text/chapter1/",ignore.case = TRUE,mode="text"))
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, "", x))
ch1Corpus <- tm_map(ch1Corpus,removeWords,c(stopwords('english')))
ch1Corpus <- tm_map(ch1Corpus,toSpace,"\"")
ch1Corpus <- tm_map(ch1Corpus,toSpace,"'")
ch1Corpus <- tm_map(ch1Corpus,toSpace,"“")
ch1Corpus <- tm_map(ch1Corpus,toSpace,"”")
ch1Corpus <- tm_map(ch1Corpus,toSpace,"”")
ch1Corpus <- tm_map(ch1Corpus,toSpace,"...")
# Get TDM with words of length 6 or greater characters
ch1TDM6 <- TermDocumentMatrix(ch1Corpus, control = list(wordlengths=c(6,Inf)))
# Biggram tokenizer
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
bigramTDM <- TermDocumentMatrix(ch1Corpus, control = list(tokenize = BigramTokenizer, wordlengths=c(6,Inf)))
bigramTDM <- removeSparseTerms(bigramTDM,sparse=0.75)
inspect(bigramTDM)
plot(bigramTDM, terms = findFreqTerms(bigramTDM, lowfreq = 2)[1:10], corThreshold = 0.2)
plot(bigramTDM, terms = findFreqTerms(bigramTDM, lowfreq = 6)[1:10], corThreshold = 0.2)
plot(bigramTDM, terms = findFreqTerms(bigramTDM, lowfreq = 6)[1:20], corThreshold = 0.2)
plot(bigramTDM, terms = findFreqTerms(bigramTDM, lowfreq = 6)[1:30], corThreshold = 0.2)
findFreqTerms(bigramTDM, lowfreq = 6)
findFreqTerms(bigramTDM, lowfreq = 3)
ch1Corpus <- VCorpus(DirSource("text/chapter1/",ignore.case = TRUE,mode="text"))
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, "", x))
ch1Corpus <- tm_map(ch1Corpus,removeWords,c(stopwords('english')))
ch1Corpus <- tm_map(ch1Corpus,toSpace,"\"")
ch1Corpus <- tm_map(ch1Corpus,toSpace,"'")
ch1Corpus <- tm_map(ch1Corpus,toSpace,"“")
ch1Corpus <- tm_map(ch1Corpus,toSpace,"”")
ch1Corpus <- tm_map(ch1Corpus,toSpace,"”")
ch1Corpus <- tm_map(ch1Corpus,toSpace,"...")
# Get TDM with words of length 6 or greater characters
ch1TDM6 <- TermDocumentMatrix(ch1Corpus, control = list(wordlengths=c(6,Inf)))
# Biggram tokenizer
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
bigramTDM <- TermDocumentMatrix(ch1Corpus, control = list(tokenize = BigramTokenizer, wordlengths=c(6,Inf)))
inspect(bigramTDM)
findFreqTerms(bigramTDM, lowfreq = 3)
findFreqTerms(bigramTDM, lowfreq = 2)
plot(bigramTDM, terms = findFreqTerms(bigramTDM, lowfreq = 6)[1:30], corThreshold = 0.2)
findFreqTerms(bigramTDM, lowfreq = 1)
findFreqTerms(bigramTDM, lowfreq = 1)
plot(bigramTDM, terms = findFreqTerms(bigramTDM, lowfreq = 6)[1:30], corThreshold = 0.2)
ch1Corpus <- VCorpus(DirSource("text/chapter1/",ignore.case = TRUE,mode="text"))
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, "", x))
ch1Corpus <- tm_map(ch1Corpus,removeWords,c(stopwords('english')))
ch1Corpus <- tm_map(ch1Corpus,toSpace,"\"")
ch1Corpus <- tm_map(ch1Corpus,toSpace,"'")
ch1Corpus <- tm_map(ch1Corpus,toSpace,"“")
ch1Corpus <- tm_map(ch1Corpus,toSpace,"”")
ch1Corpus <- tm_map(ch1Corpus,toSpace,"”")
ch1Corpus <- tm_map(ch1Corpus,toSpace,"...")
# Get TDM with words of length 6 or greater characters
ch1TDM6 <- TermDocumentMatrix(ch1Corpus, control = list(wordlengths=c(6,Inf)))
# Biggram tokenizer
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
bigramTDM <- TermDocumentMatrix(ch1Corpus, control = list(tokenize = BigramTokenizer, wordlengths=c(6,Inf)))
inspect(bigramTDM)
findFreqTerms(bigramTDM, lowfreq = 1)
plot(bigramTDM, terms = findFreqTerms(bigramTDM, lowfreq = 6)[1:30], corThreshold = 0.2)
# Trigram tokenizer
TrigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3))
ch1Corpus <- VCorpus(DirSource("text/chapter1/",ignore.case = TRUE,mode="text"))
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, "", x))
ch1Corpus <- tm_map(ch1Corpus,toSpace,"\"")
ch1Corpus <- tm_map(ch1Corpus,toSpace,"'")
ch1Corpus <- tm_map(ch1Corpus,toSpace,"“")
ch1Corpus <- tm_map(ch1Corpus,toSpace,"”")
ch1Corpus <- tm_map(ch1Corpus,toSpace,"”")
ch1Corpus <- tm_map(ch1Corpus,toSpace,"...")
# Get TDM with words of length 6 or greater characters
ch1TDM6 <- TermDocumentMatrix(ch1Corpus, control = list(wordlengths=c(6,Inf)))
# Biggram tokenizer
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
bigramTDM <- TermDocumentMatrix(ch1Corpus, control = list(tokenize = BigramTokenizer, wordlengths=c(6,Inf)))
inspect(bigramTDM)
findFreqTerms(bigramTDM, lowfreq = 1)
plot(bigramTDM, terms = findFreqTerms(bigramTDM, lowfreq = 6)[1:30], corThreshold = 0.2)
findFreqTerms(ch1TDM6, lowfreq = 1)
findFreqTerms(ch1TDM6, lowfreq = 2)
ch1Corpus <- VCorpus(DirSource("text/chapter1/",ignore.case = TRUE,mode="text"))
# Get TDM with words of length 6 or greater characters
ch1TDM6 <- TermDocumentMatrix(ch1Corpus, control = list(wordlengths=c(6,Inf)))
# Biggram tokenizer
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
bigramTDM <- TermDocumentMatrix(ch1Corpus, control = list(tokenize = BigramTokenizer, wordlengths=c(6,Inf)))
inspect(bigramTDM)
findFreqTerms(ch1TDM6, lowfreq = 2)
plot(bigramTDM, terms = findFreqTerms(bigramTDM, lowfreq = 6)[1:30], corThreshold = 0.2)
ch1Corpus <- VCorpus(DirSource("text/chapter1/",ignore.case = TRUE,mode="text"))
# Get TDM with words of length 6 or greater characters
ch1TDM6 <- TermDocumentMatrix(ch1Corpus, control = list(wordlengths=c(6,Inf)))
# Biggram tokenizer
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
bigramTDM <- TermDocumentMatrix(ch1Corpus, control = list(tokenize = BigramTokenizer, wordlengths=c(6,Inf)))
inspect(bigramTDM)
findFreqTerms(ch1TDM6, lowfreq = 2)
plot(bigramTDM, terms = findFreqTerms(bigramTDM, lowfreq = 6)[1:30], corThreshold = 0.2)
plot(bigramTDM, terms = findFreqTerms(bigramTDM, lowfreq = 6)[1:30], corThreshold = 0.2)
plot(bigramTDM, terms = findFreqTerms(bigramTDM, lowfreq = 2)[1:30], corThreshold = 0.2)
# Trigram tokenizer
TrigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3))
trigramTDM <- TermDocumentMatrix(ch1Corpus, control = list(tokenize = TrigramTokenizer, wordlengths=c(6,Inf)))
inspect(trigramTDM)
plot(bigramTDM, terms = findFreqTerms(bigramTDM, lowfreq = 2)[1:10], corThreshold = 0.2)
# Trigram tokenizer
TrigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3))
trigramTDM <- TermDocumentMatrix(ch1Corpus, control = list(tokenize = TrigramTokenizer, wordlengths=c(6,Inf)))
inspect(trigramTDM)
plot(trigramTDM, terms = findFreqTerms(trigramTDM, lowfreq = 2)[1:10], corThreshold = 0.5)
plot(trigramTDM, terms = findFreqTerms(trigramTDM, lowfreq = 2)[1:10], corThreshold = 0.2)
plot(bigramTDM, terms = findFreqTerms(bigramTDM, lowfreq = 2)[1:2], corThreshold = 0.2)
ch1Corpus <- VCorpus(DirSource("text/chapter1/",ignore.case = TRUE,mode="text"))
ch1Corpus <- tm_map(ch1Corpus,toSpace,"’")
# Get TDM with words of length 6 or greater characters
ch1TDM6 <- TermDocumentMatrix(ch1Corpus, control = list(wordlengths=c(6,Inf)))
# Biggram tokenizer
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
bigramTDM <- TermDocumentMatrix(ch1Corpus, control = list(tokenize = BigramTokenizer, wordlengths=c(6,Inf)))
inspect(bigramTDM)
plot(bigramTDM, terms = findFreqTerms(bigramTDM, lowfreq = 2)[1:2], corThreshold = 0.2)
ch1Corpus <- VCorpus(DirSource("text/chapter1/",ignore.case = TRUE,mode="text"))
ch1Corpus <- tm_map(ch1Corpus,toSpace,"’")
ch1Corpus <- tm_map(ch1Corpus,toSpace,"”")
# Get TDM with words of length 6 or greater characters
ch1TDM6 <- TermDocumentMatrix(ch1Corpus, control = list(wordlengths=c(6,Inf)))
# Biggram tokenizer
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
bigramTDM <- TermDocumentMatrix(ch1Corpus, control = list(tokenize = BigramTokenizer, wordlengths=c(6,Inf)))
inspect(bigramTDM)
plot(bigramTDM, terms = findFreqTerms(bigramTDM, lowfreq = 2)[1:2], corThreshold = 0.2)
ch1Corpus <- VCorpus(DirSource("text/chapter1/",ignore.case = TRUE,mode="text"))
ch1Corpus <- tm_map(ch1Corpus,toSpace,"’")
ch1Corpus <- tm_map(ch1Corpus,toSpace,"”")
ch1Corpus <- tm_map(ch1Corpus,toSpace,"“")
# Get TDM with words of length 6 or greater characters
ch1TDM6 <- TermDocumentMatrix(ch1Corpus, control = list(wordlengths=c(6,Inf)))
# Biggram tokenizer
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
bigramTDM <- TermDocumentMatrix(ch1Corpus, control = list(tokenize = BigramTokenizer, wordlengths=c(6,Inf)))
inspect(bigramTDM)
plot(bigramTDM, terms = findFreqTerms(bigramTDM, lowfreq = 2)[1:2], corThreshold = 0.2)
plot(bigramTDM, terms = findFreqTerms(bigramTDM, lowfreq = 2)[1:3], corThreshold = 0.2)
plot(bigramTDM, terms = findFreqTerms(bigramTDM, lowfreq = 2)[1:5], corThreshold = 0.2)
plot(bigramTDM, terms = findFreqTerms(bigramTDM, lowfreq = 2)[1:5], corThreshold = 0.5)
plot(bigramTDM, terms = findFreqTerms(bigramTDM, lowfreq = 2)[1:10], corThreshold = 0.5)
plot(bigramTDM, terms = findFreqTerms(bigramTDM, lowfreq = 2)[1], corThreshold = 0.5)
plot(bigramTDM, terms = findFreqTerms(bigramTDM, lowfreq = 2)[2], corThreshold = 0.5)
plot(bigramTDM, terms = findFreqTerms(bigramTDM, lowfreq = 2)[2], corThreshold = 0.5)
plot(bigramTDM, terms = findFreqTerms(bigramTDM, lowfreq = 2)[3], corThreshold = 0.5)
ch1Corpus <- tm_map(ch1Corpus,toSpace,"’")
ch1Corpus <- VCorpus(DirSource("text/chapter1/",ignore.case = TRUE,mode="text"))
ch1Corpus <- tm_map(ch1Corpus,toSpace,"’")
ch1Corpus <- tm_map(ch1Corpus,toSpace,"”")
ch1Corpus <- tm_map(ch1Corpus,toSpace,"“")
ch1Corpus <- tm_map(ch1Corpus,toSpace," a ")
# Get TDM with words of length 6 or greater characters
ch1TDM6 <- TermDocumentMatrix(ch1Corpus, control = list(wordlengths=c(6,Inf)))
# Biggram tokenizer
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
bigramTDM <- TermDocumentMatrix(ch1Corpus, control = list(tokenize = BigramTokenizer, wordlengths=c(6,Inf)))
inspect(bigramTDM)
plot(bigramTDM, terms = findFreqTerms(bigramTDM, lowfreq = 2)[3], corThreshold = 0.5)
plot(bigramTDM, terms = findFreqTerms(bigramTDM, lowfreq = 2)[1:4], corThreshold = 0.5)
plot(bigramTDM, terms = findFreqTerms(bigramTDM, lowfreq = 2)[1:4], corThreshold = 0.5)
plot(bigramTDM, terms = findFreqTerms(bigramTDM, lowfreq = 2)[1:4], corThreshold = 0.75)
plot(bigramTDM, terms = findFreqTerms(bigramTDM, lowfreq = 2)[1:4], corThreshold = 0.75)
ch1Corpus <- VCorpus(DirSource("text/chapter1/",ignore.case = TRUE,mode="text"))
ch1Corpus <- tm_map(ch1Corpus,toSpace,"’")
ch1Corpus <- tm_map(ch1Corpus,toSpace,"”")
ch1Corpus <- tm_map(ch1Corpus,toSpace,"“")
ch1Corpus <- tm_map(ch1Corpus,toSpace," a ")
# Get TDM with words of length 6 or greater characters
ch1TDM6 <- TermDocumentMatrix(ch1Corpus, control = list(wordlengths=c(6,Inf)))
# Biggram tokenizer
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
bigramTDM <- TermDocumentMatrix(ch1Corpus, control = list(tokenize = BigramTokenizer, wordlengths=c(6,Inf)))
inspect(bigramTDM)
plot(bigramTDM, terms = findFreqTerms(bigramTDM, lowfreq = 2)[1:4], corThreshold = 0.75)
ch1Corpus <- VCorpus(DirSource("text/chapter1/",ignore.case = TRUE,mode="text"))
ch1Corpus <- tm_map(ch1Corpus,toSpace,"’")
ch1Corpus <- tm_map(ch1Corpus,toSpace,"”")
ch1Corpus <- tm_map(ch1Corpus,toSpace,"“")
ch1Corpus <- tm_map(ch1Corpus,toSpace," a ")
ch1Corpus <- tm_map(ch1Corpus,toSpace," mr ")
# Get TDM with words of length 6 or greater characters
ch1TDM6 <- TermDocumentMatrix(ch1Corpus, control = list(wordlengths=c(6,Inf)))
# Biggram tokenizer
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
bigramTDM <- TermDocumentMatrix(ch1Corpus, control = list(tokenize = BigramTokenizer, wordlengths=c(6,Inf)))
inspect(bigramTDM)
plot(bigramTDM, terms = findFreqTerms(bigramTDM, lowfreq = 2)[1:4], corThreshold = 0.75)
plot(bigramTDM, terms = findFreqTerms(bigramTDM, lowfreq = 2)[1:4], corThreshold = 0.75)
ch1Corpus <- tm_map(ch1Corpus,toSpace," a ")
ch1Corpus <- VCorpus(DirSource("text/chapter1/",ignore.case = TRUE,mode="text"))
ch1Corpus <- tm_map(ch1Corpus,toSpace,"’")
ch1Corpus <- tm_map(ch1Corpus,toSpace,"”")
ch1Corpus <- tm_map(ch1Corpus,toSpace,"“")
ch1Corpus <- tm_map(ch1Corpus,toSpace," a ")
ch1Corpus <- tm_map(ch1Corpus,toSpace,"mr ")
# Get TDM with words of length 6 or greater characters
ch1TDM6 <- TermDocumentMatrix(ch1Corpus, control = list(wordlengths=c(6,Inf)))
# Biggram tokenizer
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
bigramTDM <- TermDocumentMatrix(ch1Corpus, control = list(tokenize = BigramTokenizer, wordlengths=c(6,Inf)))
inspect(bigramTDM)
plot(bigramTDM, terms = findFreqTerms(bigramTDM, lowfreq = 2)[1:4], corThreshold = 0.75)
ch1Corpus <- VCorpus(DirSource("text/chapter1/",ignore.case = TRUE,mode="text"))
ch1Corpus <- tm_map(ch1Corpus,toSpace,"’")
ch1Corpus <- tm_map(ch1Corpus,toSpace,"”")
ch1Corpus <- tm_map(ch1Corpus,toSpace,"“")
ch1Corpus <- tm_map(ch1Corpus,toSpace," a ")
ch1Corpus <- tm_map(ch1Corpus,removeWords,c("and","the","for","of"))
# Get TDM with words of length 6 or greater characters
ch1TDM6 <- TermDocumentMatrix(ch1Corpus, control = list(wordlengths=c(6,Inf)))
# Biggram tokenizer
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
ch1Corpus <- VCorpus(DirSource("text/chapter1/",ignore.case = TRUE,mode="text"))
ch1Corpus <- tm_map(ch1Corpus,toSpace,"’")
ch1Corpus <- tm_map(ch1Corpus,toSpace,"”")
ch1Corpus <- tm_map(ch1Corpus,toSpace,"“")
ch1Corpus <- tm_map(ch1Corpus,toSpace," a ")
ch1Corpus <- tm_map(ch1Corpus,removeWords,c("and","the","for","of"))
# Biggram tokenizer
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
bigramTDM <- TermDocumentMatrix(ch1Corpus, control = list(tokenize = BigramTokenizer, wordlengths=c(6,Inf)))
inspect(bigramTDM)
plot(bigramTDM, terms = findFreqTerms(bigramTDM, lowfreq = 2)[1:4], corThreshold = 0.75)
ch1Corpus <- VCorpus(DirSource("text/chapter1/",ignore.case = TRUE,mode="text"))
ch1Corpus <- tm_map(ch1Corpus,toSpace,"’")
ch1Corpus <- tm_map(ch1Corpus,toSpace,"”")
ch1Corpus <- tm_map(ch1Corpus,toSpace,"“")
ch1Corpus <- tm_map(ch1Corpus,toSpace," a ")
ch1Corpus <- tm_map(ch1Corpus,removeWords,c("and","the","for","of"))
# Biggram tokenizer
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
bigramTDM <- TermDocumentMatrix(ch1Corpus, control = list(tokenize = BigramTokenizer, wordlengths=c(6,Inf)))
inspect(bigramTDM)
plot(bigramTDM, terms = findFreqTerms(bigramTDM, lowfreq = 2)[1:4], corThreshold = 0.75)
ch1Corpus <- VCorpus(DirSource("text/chapter1/",ignore.case = TRUE,mode="text"))
ch1Corpus <- tm_map(ch1Corpus,toSpace,"’")
ch1Corpus <- tm_map(ch1Corpus,toSpace,"”")
ch1Corpus <- tm_map(ch1Corpus,toSpace,"“")
ch1Corpus <- tm_map(ch1Corpus,toSpace," a ")
ch1Corpus <- tm_map(ch1Corpus,removeWords,c("and","the","for","of","that"))
# Biggram tokenizer
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
bigramTDM <- TermDocumentMatrix(ch1Corpus, control = list(tokenize = BigramTokenizer, wordlengths=c(6,Inf)))
inspect(bigramTDM)
plot(bigramTDM, terms = findFreqTerms(bigramTDM, lowfreq = 2)[1:4], corThreshold = 0.75)
ch1Corpus <- VCorpus(DirSource("text/chapter1/",ignore.case = TRUE,mode="text"))
ch1Corpus <- tm_map(ch1Corpus,toSpace,"’")
ch1Corpus <- tm_map(ch1Corpus,toSpace,"”")
ch1Corpus <- tm_map(ch1Corpus,toSpace,"“")
ch1Corpus <- tm_map(ch1Corpus,toSpace," a ")
ch1Corpus <- tm_map(ch1Corpus,removeWords,c("and","the","for","of","that"))
# Biggram tokenizer
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
bigramTDM <- TermDocumentMatrix(ch1Corpus, control = list(tokenize = BigramTokenizer, wordlengths=c(6,Inf)))
inspect(bigramTDM)
plot(bigramTDM, terms = findFreqTerms(bigramTDM, lowfreq = 2)[1:4], corThreshold = 0.75)
plot(bigramTDM, terms = findFreqTerms(bigramTDM, lowfreq = 2)[1:4], corThreshold = 0.75)
# Trigram tokenizer
TrigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3))
trigramTDM <- TermDocumentMatrix(ch1Corpus, control = list(tokenize = TrigramTokenizer, wordlengths=c(6,Inf)))
inspect(trigramTDM)
plot(trigramTDM, terms = findFreqTerms(trigramTDM, lowfreq = 2)[1:2], corThreshold = 0.2)
# Trigram tokenizer
TrigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3))
trigramTDM <- TermDocumentMatrix(ch1Corpus, control = list(tokenize = TrigramTokenizer, wordlengths=c(6,Inf)))
inspect(trigramTDM)
plot(trigramTDM, terms = findFreqTerms(trigramTDM, lowfreq = 2)[1:2], corThreshold = 0.2)
ch1Corpus <- VCorpus(DirSource("text/chapter1/",ignore.case = TRUE,mode="text"))
ch1Corpus <- tm_map(ch1Corpus,toSpace,"’")
ch1Corpus <- tm_map(ch1Corpus,toSpace,"”")
ch1Corpus <- tm_map(ch1Corpus,toSpace,"“")
ch1Corpus <- tm_map(ch1Corpus,toSpace," a ")
ch1Corpus <- tm_map(ch1Corpus,removeWords,c("and","the","for","of","that","but","i","have","after","all"))
# Biggram tokenizer
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
bigramTDM <- TermDocumentMatrix(ch1Corpus, control = list(tokenize = BigramTokenizer, wordlengths=c(6,Inf)))
inspect(bigramTDM)
plot(bigramTDM, terms = findFreqTerms(bigramTDM, lowfreq = 2)[1:4], corThreshold = 0.75)
# Trigram tokenizer
TrigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3))
trigramTDM <- TermDocumentMatrix(ch1Corpus, control = list(tokenize = TrigramTokenizer, wordlengths=c(6,Inf)))
inspect(trigramTDM)
plot(trigramTDM, terms = findFreqTerms(trigramTDM, lowfreq = 2)[1:2], corThreshold = 0.2)
plot(bigramTDM, terms = findFreqTerms(bigramTDM, lowfreq = 2)[1:4], corThreshold = 0.75)
ch1Corpus <- VCorpus(DirSource("text/chapter1/",ignore.case = TRUE,mode="text"))
ch1Corpus <- tm_map(ch1Corpus,toSpace,"’")
ch1Corpus <- tm_map(ch1Corpus,toSpace,"”")
ch1Corpus <- tm_map(ch1Corpus,toSpace,"“")
ch1Corpus <- tm_map(ch1Corpus,toSpace," a ")
# Biggram tokenizer
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
bigramTDM <- TermDocumentMatrix(ch1Corpus, control = list(tokenize = BigramTokenizer, wordlengths=c(6,Inf)))
inspect(bigramTDM)
plot(bigramTDM, terms = findFreqTerms(bigramTDM, lowfreq = 2)[1:4], corThreshold = 0.75)
# Trigram tokenizer
TrigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3))
trigramTDM <- TermDocumentMatrix(ch1Corpus, control = list(tokenize = TrigramTokenizer, wordlengths=c(6,Inf)))
inspect(trigramTDM)
plot(bigramTDM, terms = findFreqTerms(bigramTDM, lowfreq = 2)[1:4], corThreshold = 0.75)
# Trigram tokenizer
TrigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3))
# 1h. Process the text from the data document using corpusTools, stringi, quanteda,
# and tidytext. Describe the methods you use, the results you get, and what you understand
# about the theme of the book.
# Import data document
data <- read_lines("DrJekyllAndMrHyde.txt", skip = 0, n_max = -1L)
data
data("DrJekyllandMrHyde.txt")
data(DirSource("DrJekyllandMrHyde.txt"))
help(corpusTools)
help(corpustools)
help("corpusTools")
library(corpusTools)
install.packages("corpusTools")
install.packages("corpustools")
library(corpustools)
install.packages("readtext")
library(readtext)
library(topicmodels)
library(spacyr)
dataDoc <- read_lines("DrJekyllAndMrHyde.txt", skip = 0, n_max = -1L)
rt <- readtext("DrJekyllAndMrHyde.txt",text_field="texts")
rt
dataDoc <- read_lines("DrJekyllAndMrHyde.txt", skip = 0, n_max = -1L)
# Stringi
dataDoc <- stri_trans_tolower(dataDoc)
# Strip surrounding white space
dataDoc <- stri_replace_all(x,"",regex = "<.*?>")
# Strip surrounding white space
dataDoc <- stri_replace_all(dataDoc,"",regex = "<.*?>")
# Strip surrounding white space
dataDoc <- stri_trim(x)
# Strip surrounding white space
dataDoc <- stri_trim(dataDoc)
dataDoc
# Quanteda
# Tokenization
tokens <- tokens(dataDoc)
dataDoc <- toString(read_lines("DrJekyllAndMrHyde.txt", skip = 0, n_max = -1L))
# Stringi
# Transform to lower case
dataDoc <- stri_trans_tolower(dataDoc)
# Remove tags
dataDoc <- stri_replace_all(dataDoc,"",regex = "<.*?>")
# Strip surrounding white space
dataDoc <- stri_trim(dataDoc)
dataDoc
# Quanteda
# Tokenization
tokens <- tokens(dataDoc)
# Quanteda
# Tokenization
tokens <- tokens(dataDoc)
# Quanteda
# Tokenization
dataDoc
# Quanteda
# Tokenization
is.String(dataDoc)
# Quanteda
# Tokenization
dataDoc
# Quanteda
# Tokenization
dataDoc <- toString(dataDoc)
tokens <- tokens(dataDoc)
tk <- tokens(dataDoc)
# Quanteda
# Tokenization
dataDoc <- toString(dataDoc)
tk <- tokens(dataDoc)
# Quanteda
# Tokenization
dataDoc
# Quanteda
# Tokenization
str(dataDoc)
# Strip surrounding white space
dataDoc <- str_trim(dataDoc)
# Stringi
# Transform to lower case
dataDoc <- str_trans_tolower(dataDoc)
# Remove tags
dataDoc <- str_replace_all(dataDoc,"",regex = "<.*?>")
# Stringi
# Transform to lower case
dataDoc <- stri_trans_tolower(dataDoc)
# Remove tags
dataDoc <- str_replace_all(dataDoc,"",regex = "<.*?>")
# Remove tags
dataDoc <- stri_replace_all(dataDoc,"",regex = "<.*?>")
# Strip surrounding white space
dataDoc <- str_trim(dataDoc)
dataDoc
# Reduce repeated whitespace inside string
dataDoc <- str_squish(dataDoc)
dataDoc
dataDoc1 <- stri_enc_toutf8(dataDoc, is_unknown_8bit = FALSE, validate = FALSE)
dataDoc <- toString(dataDoc)
tk <- tokens(dataDoc1)
# Quanteda
# Tokenization
str(dataDoc1)
dataDoc1 <- toString(dataDoc1)
tk <- tokens(dataDoc1)
tk <- quanteda::tokens(dataDoc)
tk
tk
# Quanteda
# Tokenization
toks <- quanteda::tokens(dataDoc)
toks
toks <- toks_tolower(toks)
toks <- tokens_tolower(toks)
toks
toks <- tokens_wordstem(toks)
toks
# Quanteda
# Tokenization
toks <- quanteda::tokens(dataDoc)
toks <- tokens_tolower(toks)
toks
# Remove stopwords
sw <- stopwords("english")
head(sw)
tokens_remove(toks,sw)
dtm <- dfm(dataDoc,tolower=TRUE,stem=TRUE,remove=stopwords(("english")))
dtm
dtm <- dfm(dataDoc,tolower=TRUE,stem=FALSE,remove=stopwords(("english")))
dtm
dtm <- dfm(dataDoc,tolower=TRUE,stem=FALSE,remove=stopwords(("english")))
dtm
rt <- readtext("DrJekyllAndMrHyde.txt",text_field="texts")
dtm <- dfm(rt,tolower=TRUE,stem=FALSE,remove=stopwords(("english")))
dtm <- dfm(toks,tolower=TRUE,stem=FALSE,remove=stopwords(("english")))
dtm
fulltext <- corpus(rt)
dtm <- dfm(fulltext, tolower = TRUE, stem = TRUE,
remove_punct = TRUE,remove = stopwords("english"))
dtm
rt <- readtext("/text/",text_field="texts")
rt <- readtext("/text",text_field="texts")
rt <- readtext("text/",text_field="texts")
rt <- readtext("/DrJekyllAndMrHyde.txt",text_field="texts")
rt <- readtext("/DrJekyllAndMrHyde.txt",text_field="texts")
rt <- readtext("DrJekyllAndMrHyde.txt",text_field="texts")
rt <- readtext("text/",text_field="texts")
rt <- readtext("text/",text_field="texts",check.rows=TRUE)
readtext_options()
rt <- readtext("text/",text_field="texts")
help(readtext)
rt <- readtext("text/*.txt",text_field="texts")
rt <- readtext("text/*.txt",text_field="texts")
rt <- readtext("text/*.txt",text_field="texts")
fulltext <- corpus(rt)
dtm <- dfm(fulltext, tolower = TRUE, stem = TRUE,
remove_punct = TRUE,remove = stopwords("english"))
dtm
rt <- readtext("text/*.txt",docvarsfrom="filepaths")
rt
dtm <- dfm(fulltext, tolower = TRUE, stem = TRUE,
remove_punct = TRUE,remove = stopwords("english"))
fulltext <- corpus(rt)
dtm <- dfm(fulltext, tolower = TRUE, stem = TRUE,
remove_punct = TRUE,remove = stopwords("english"))
dtm
str(dtm)
head(dtm)
head(dtm)
str(dtm)
dtm@
dtm@settings
dtm@weightTf
dtm@settings
dtm@weightDf
dtm@smooth
dtm@ngrams
dtm@skip
dtm@docvars
dtm@Dimnames
dtm@factors
dtm@docvars
dtm@skip
dtm@docvars
dtm@Dimnames
# Document Frequency
doc_freq <- docfreq(dtm)
# Terms with frequency >= 6
dtm <- dtm[, doc_freq >= 6]      # select terms with doc_freq >= 2
dtm <- dfm_weight(dtm, "tfidf")  # weight the features using tf-idf
dtm <- dfm_weight(dtm, "dfm_tfidf")  # weight the features using tf-idf
dfm_tfidf(dtm)
dtm <- dfm(fulltext, tolower = TRUE, stem = TRUE,
remove_punct = TRUE,remove = stopwords("english"))
dtm@docvars
dtm@Dimnames
# Document Frequency
doc_freq <- docfreq(dtm)
doc_freq
# Terms with frequency >= 6
dtm6 <- dtm[, doc_freq >= 6]
# TF-IDF
dtm1 <- dfm_weight(dtm, "tfidf")
dfm_tfidf(dtm)
## Warning: scheme = "tfidf" is deprecated; use dfm_tfidf(x) instead
head(dtm)
