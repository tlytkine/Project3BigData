## Download and install the package
install.packages("igraph")
install.packages("stringr", dependencies = TRUE)
install.packages("quanteda", dependencies = T)
library(stringr)
library(quanteda)
library(tm)
text <- read.delim("DrJekyllAndMrHyde.txt")
setwd("~/Documents/GWU/Spring 2019/CSCI 3907/Projects/Project3/test")
text <- read.delim("DrJekyllAndMrHyde.txt")
text
book <- VCorpus(DirSource("test", ignore.case=TRUE, mode="text"))
book <- VCorpus(DirSource(".", ignore.case=TRUE, mode="text"))
View(book)
book
Clean_String <- function(string){
# Lowercase
temp <- tolower(string)
# Remove everything that is not a number or letter (may want to keep more
# stuff in your actual analyses).
temp <- stringr::str_replace_all(temp,"[^a-zA-Z\\s]", " ")
# Shrink down to just one white space
temp <- stringr::str_replace_all(temp,"[\\s]+", " ")
# Split it
temp <- stringr::str_split(temp, " ")[[1]]
# Get rid of trailing "" if necessary
indexes <- which(temp == "")
if(length(indexes) > 0){
temp <- temp[-indexes]
}
return(temp)
}
# function to clean text
Clean_Text_Block <- function(text){
# Get rid of blank lines
indexes <- which(text == "")
if (length(indexes) > 0) {
text <- text[-indexes]
}
# See if we are left with any valid text:
if (length(text) == 0) {
cat("There was no text in this document! \n")
to_return <- list(num_tokens = 0,
unique_tokens = 0,
text = "")
} else {
# If there is valid text, process it.
# Loop through the lines in the text and combine them:
clean_text <- NULL
for (i in 1:length(text)) {
# add them to a vector
clean_text <- c(clean_text, Clean_String(text[i]))
}
# Calculate the number of tokens and unique tokens and return them in a
# named list object.
num_tok <- length(clean_text)
num_uniq <- length(unique(clean_text))
to_return <- list(num_tokens = num_tok,
unique_tokens = num_uniq,
text = clean_text)
}
return(to_return)
}
clean_speech <- Clean_Text_Block(book)
str(clean_speech)
str(book)
test1<-clean_speech[[1]]
test1
test2<-book[[1]]
test2
bookdtm <- DocumentTermMatrix(book)
bookdtm
text <- read.delim("DrJekyllAndMrHyde.txt")
text <- read.delim("text/DrJekyllAndMrHyde.txt")
text
book <- VCorpus(DirSource("text/", ignore.case=TRUE, mode="text"))
book
Clean_String <- function(string){
# Lowercase
temp <- tolower(string)
# Remove everything that is not a number or letter (may want to keep more
# stuff in your actual analyses).
temp <- stringr::str_replace_all(temp,"[^a-zA-Z\\s]", " ")
# Shrink down to just one white space
temp <- stringr::str_replace_all(temp,"[\\s]+", " ")
# Split it
temp <- stringr::str_split(temp, " ")[[1]]
# Get rid of trailing "" if necessary
indexes <- which(temp == "")
if(length(indexes) > 0){
temp <- temp[-indexes]
}
return(temp)
}
# function to clean text
Clean_Text_Block <- function(text){
# Get rid of blank lines
indexes <- which(text == "")
if (length(indexes) > 0) {
text <- text[-indexes]
}
# See if we are left with any valid text:
if (length(text) == 0) {
cat("There was no text in this document! \n")
to_return <- list(num_tokens = 0,
unique_tokens = 0,
text = "")
} else {
# If there is valid text, process it.
# Loop through the lines in the text and combine them:
clean_text <- NULL
for (i in 1:length(text)) {
# add them to a vector
clean_text <- c(clean_text, Clean_String(text[i]))
}
# Calculate the number of tokens and unique tokens and return them in a
# named list object.
num_tok <- length(clean_text)
num_uniq <- length(unique(clean_text))
to_return <- list(num_tokens = num_tok,
unique_tokens = num_uniq,
text = clean_text)
}
return(to_return)
}
clean_speech <- Clean_Text_Block(book)
str(clean_speech)
str(book)
test1<-clean_speech[[1]]
test1
test2<-book[[1]]
test2
bookdtm <- DocumentTermMatrix(book)
bookdtm
inspect(bookdtm[1:12,1:6])
inspect(bookdtm[1:12,1:])
inspect(bookdtm[1:12,1:,])
stopwords <- c(stopwords('english'))
stopwords
bookstop <- tm_map(book, removeWords, stopwords)
inspect(SATstop[1:3])
inspect(bookstop[1:3])
booktdm2 <- TermDocumentMatrix(bookstop, control = list(wordlengths=c(1,Inf)))
booktdm2
freqTerms <-findFreqTerms(booktdm2, lowfreq = 4)
freqTerms
statesAssoc <- findAssocs(booktdm2, "states", 0.5)
statesAssoc
statesAssoc <- findAssocs(booktdm2, "states", 0.5)
statesAssoc
statesAssoc <- findAssocs(booktdm2, "states", 1)
statesAssoc
termFreq <- rowSums(as.matrix(booktdm2))
termFreqSub <- subset(termFreq, termFreq >= 6)
termFreqdf <- as.data.frame(names(termFreq), freq = termFreq)
termFreqdf <- as.data.frame(names(termFreq), freq = termFreq)
termFreq
bookdtm2
booktdm2
sparsetdm2 <- removeSparseTerms(booktdm2, sparse = 0.75)
inspect(sparsetdm2)
sparsetdm2
fit <- hclust(distMatrix, method)
